// ===========================================
// OpenCL 核函数：三层全连接神经网络的前向推理
// ===========================================
// 网络结构说明（基于 model_meta.txt）:
// - linear_0: 输入层 784个神经元 -> 隐藏层1 128个神经元
// - linear_1: 隐藏层1 128个神经元 -> 隐藏层2 64个神经元
// - linear_2: 隐藏层2 64个神经元 -> 输出层 10个神经元（0-9数字分类）
// 激活函数：前两层使用ReLU，最后一层直接输出logits

// ===========================================
// 辅助函数：计算矩阵行向量与输入向量的点积
// ===========================================
// 这个函数计算矩阵第rowIndex行的向量与输入向量vec的点积
// 用于实现矩阵-向量乘法：matrix[rowIndex, :] · vec
inline float dot_row_vec(__global const float* matrix,  // 输入：权重矩阵（全局内存）
                         int rowIndex,                  // 输入：要计算的行索引
                         int numCols,                   // 输入：矩阵列数（向量维度）
                         __global const float* vec)     // 输入：输入向量（全局内存）
{
    // 初始化累加器，用于存储点积结果
    float acc = 0.0f;

    // 计算该行在矩阵中的起始位置（行主序存储）
    // matrix存储格式：[行0列0, 行0列1, ..., 行0列N-1, 行1列0, ...]
    int base = rowIndex * numCols;

    // 遍历该行的每一列，累加 matrix[rowIndex, c] * vec[c]
    for (int c = 0; c < numCols; ++c) {
        // 读取矩阵元素并与对应输入向量元素相乘后累加
        acc += matrix[base + c] * vec[c];
    }

    // 返回点积结果
    return acc;
}

// ===========================================
// 核函数1：单样本前向推理
// ===========================================
// 该核函数在一个work-item中处理一个MNIST样本的前向推理
// 输入：784维像素向量（28x28图像展平）
// 输出：10维logits向量（每个数字0-9的得分）
__kernel void mlp_forward_one(
    __global const float* restrict x,          // 输入：[784] 单个样本的像素值向量（全局内存，只读）
    __global const float* restrict W0,         // 权重：[784*128] 第一层权重矩阵（行主序：784行，128列）
    __global const float* restrict b0,         // 偏置：[128] 第一层偏置向量
    __global const float* restrict W1,         // 权重：[128*64] 第二层权重矩阵（行主序：128行，64列）
    __global const float* restrict b1,         // 偏置：[64] 第二层偏置向量
    __global const float* restrict W2,         // 权重：[64*10] 第三层权重矩阵（行主序：64行，10列）
    __global const float* restrict b2,         // 偏置：[10] 第三层偏置向量
    __global float* restrict logits            // 输出：[10] 网络输出的logits（全局内存，只写）
)
{
    // ===========================================
    // 第一隐藏层：y0 = ReLU(x * W0 + b0)
    // ===========================================
    // 输入：x (784维) -> 输出：h0 (128维)
    // 激活函数：ReLU (max(0, x))

    // 声明第一隐藏层的输出数组（私有内存）
    float h0[128];

    // 对每一输出神经元j（0-127）进行计算
    for (int j = 0; j < 128; ++j) {
        // 初始化当前神经元的加权输入累加器
        float acc = 0.0f;

        // 计算当前神经元的加权输入：sum(x[i] * W0[i,j]) for i=0 to 783
        // W0按行主序存储：W0[i*128 + j] 对应权重矩阵的第i行第j列
        for (int i = 0; i < 784; ++i) {
            // 累加：输入像素x[i]与对应权重W0[i,j]的乘积
            acc += x[i] * W0[i * 128 + j];
        }

        // 加上偏置项：weighted_input = sum(...) + bias[j]
        float v = acc + b0[j];

        // 应用ReLU激活函数：max(0, weighted_input)
        // 如果v > 0则保持原值，否则设为0
        h0[j] = v > 0.0f ? v : 0.0f;
    }

    // ===========================================
    // 第二隐藏层：y1 = ReLU(h0 * W1 + b1)
    // ===========================================
    // 输入：h0 (128维) -> 输出：h1 (64维)
    // 激活函数：ReLU

    // 声明第二隐藏层的输出数组
    float h1[64];

    // 对每一输出神经元j（0-63）进行计算
    for (int j = 0; j < 64; ++j) {
        // 初始化累加器
        float acc = 0.0f;

        // 注释：这里修正了之前的错误计算
        // 我们想要计算 out_j = sum_i h0_i * W1[i, j]
        // W1按行主序存储：W1[i*64 + j] 对应第i行第j列

        // 计算加权输入：sum(h0[i] * W1[i,j]) for i=0 to 127
        for (int i = 0; i < 128; ++i) {
            // 累加：上一层输出h0[i]与权重W1[i,j]的乘积
            acc += h0[i] * W1[i * 64 + j];
        }

        // 加上偏置并应用ReLU
        float v = acc + b1[j];
        h1[j] = v > 0.0f ? v : 0.0f;
    }

    // ===========================================
    // 输出层：logits = h1 * W2 + b2
    // ===========================================
    // 输入：h1 (64维) -> 输出：logits (10维)
    // 注意：输出层不使用激活函数，直接输出原始得分

    // 对每一输出类别j（0-9）进行计算
    for (int j = 0; j < 10; ++j) {
        // 初始化累加器
        float acc = 0.0f;

        // 计算logits：sum(h1[i] * W2[i,j]) for i=0 to 63
        // W2按行主序存储：W2[i*10 + j] 对应第i行第j列
        for (int i = 0; i < 64; ++i) {
            // 累加：上一层输出h1[i]与权重W2[i,j]的乘积
            acc += h1[i] * W2[i * 10 + j];
        }

        // 加上偏置项并写入输出
        // 注意：输出层没有激活函数，直接输出加权输入 + 偏置
        logits[j] = acc + b2[j];
    }
}

// ===========================================
// 核函数2：批量前向推理
// ===========================================
// 该核函数支持批量处理多个样本，每个work-item处理一个样本
// 适用于一次性推理多个MNIST图像的高效场景
__kernel void mlp_forward_batch(
    __global const float* restrict X,          // 输入：[N,784] N个样本的像素矩阵（全局内存，只读）
    int N,                                     // 输入：样本数量N
    __global const float* restrict W0,         // 权重：[784*128] 第一层权重矩阵
    __global const float* restrict b0,         // 偏置：[128] 第一层偏置向量
    __global const float* restrict W1,         // 权重：[128*64] 第二层权重矩阵
    __global const float* restrict b1,         // 偏置：[64] 第二层偏置向量
    __global const float* restrict W2,         // 权重：[64*10] 第三层权重矩阵
    __global const float* restrict b2,         // 偏置：[10] 第三层偏置向量
    __global float* restrict Out               // 输出：[N,10] N个样本的logits矩阵（全局内存，只写）
)
{
    // ===========================================
    // 获取当前work-item的全局ID（样本索引）
    // ===========================================
    int gid = get_global_id(0);  // 当前work-item处理的样本索引

    // 如果当前work-item的ID超出样本数量，直接返回（边界检查）
    if (gid >= N) return;

    // ===========================================
    // 定位当前样本在输入/输出缓冲区中的位置
    // ===========================================
    // 计算当前样本的输入向量起始地址
    // X是按行主序存储的：[sample0_pixel0, sample0_pixel1, ..., sample0_pixel783,
    //                      sample1_pixel0, sample1_pixel1, ..., sample1_pixel783, ...]
    const __global float* x = X + gid * 784;

    // 计算当前样本的输出向量起始地址
    // Out也是按行主序存储的
    __global float* logits = Out + gid * 10;

    // ===========================================
    // 第一隐藏层计算
    // ===========================================
    float h0[128];  // 第一隐藏层输出
    for (int j = 0; j < 128; ++j) {
        float acc = 0.0f;
        // 矩阵乘法：h0[j] = sum(x[i] * W0[i,j])
        for (int i = 0; i < 784; ++i) {
            acc += x[i] * W0[i * 128 + j];
        }
        float v = acc + b0[j];
        h0[j] = v > 0.0f ? v : 0.0f;  // ReLU激活
    }

    // ===========================================
    // 第二隐藏层计算
    // ===========================================
    float h1[64];  // 第二隐藏层输出
    for (int j = 0; j < 64; ++j) {
        float acc = 0.0f;
        // 矩阵乘法：h1[j] = sum(h0[i] * W1[i,j])
        for (int i = 0; i < 128; ++i) {
            acc += h0[i] * W1[i * 64 + j];
        }
        float v = acc + b1[j];
        h1[j] = v > 0.0f ? v : 0.0f;  // ReLU激活
    }

    // ===========================================
    // 输出层计算
    // ===========================================
    for (int j = 0; j < 10; ++j) {
        float acc = 0.0f;
        // 矩阵乘法：logits[j] = sum(h1[i] * W2[i,j])
        for (int i = 0; i < 64; ++i) {
            acc += h1[i] * W2[i * 10 + j];
        }
        // 直接写入输出缓冲区，无激活函数
        Out[gid * 10 + j] = acc + b2[j];
    }
}


